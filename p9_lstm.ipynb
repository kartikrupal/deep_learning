{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNv3hKLKrbwIUBndnDIimgL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartikrupal/deep_learning/blob/main/p9_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcKHCIHELuKt",
        "outputId": "4a31e7f7-f52f-4030-e92f-cf136de82bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 2s/step - acc: 0.6715 - loss: 0.5895 - val_acc: 0.8400 - val_loss: 0.3935\n",
            "Epoch 2/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 2s/step - acc: 0.8672 - loss: 0.3259 - val_acc: 0.8578 - val_loss: 0.3499\n",
            "Epoch 3/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 2s/step - acc: 0.8966 - loss: 0.2706 - val_acc: 0.8352 - val_loss: 0.3879\n",
            "Epoch 4/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 2s/step - acc: 0.9100 - loss: 0.2444 - val_acc: 0.7582 - val_loss: 0.4925\n",
            "Epoch 5/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 2s/step - acc: 0.9031 - loss: 0.2480 - val_acc: 0.8394 - val_loss: 0.3973\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584ms/step\n",
            "Text: This film filled me with joy and optimism! => Predicted sentiment: happy\n",
            "Text: A tedious, poorly executed mess of a movie. => Predicted sentiment: sad\n",
            "Text: The cinematography was competent, but the script was unpolished. => Predicted sentiment: neutral\n",
            "Text: I've never seen anything so profoundly moving in years. => Predicted sentiment: happy\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = 10000\n",
        "maxlen = 500\n",
        "embedding_dim = 128\n",
        "lstm_units = 64\n",
        "\n",
        "# Load the data\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Pad the sequences\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Model creation\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size + 4, output_dim=embedding_dim))\n",
        "model.add(LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.2)\n",
        "\n",
        "# Get the word index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Sentiment prediction function\n",
        "def predict_sentiments(sentences):\n",
        "    encoded = []\n",
        "    for sentence in sentences:\n",
        "        tokens = text_to_word_sequence(sentence.lower())\n",
        "        seq = []\n",
        "        for token in tokens:\n",
        "            idx = word_index.get(token, 0)\n",
        "            if 1 <= idx < vocab_size:\n",
        "                seq.append(idx + 3)\n",
        "            else:\n",
        "                seq.append(2)\n",
        "        encoded.append(seq)\n",
        "    padded = pad_sequences(encoded, maxlen=maxlen)\n",
        "    probs = model.predict(np.array(padded))\n",
        "    return ['happy' if p > 0.7 else 'sad' if p < 0.3 else 'neutral' for p in probs.flatten()]\n",
        "\n",
        "# Example sentences\n",
        "sentences = [\n",
        "    \"This film filled me with joy and optimism!\",\n",
        "    \"A tedious, poorly executed mess of a movie.\",\n",
        "    \"The cinematography was competent, but the script was unpolished.\",\n",
        "    \"I've never seen anything so profoundly moving in years.\"\n",
        "]\n",
        "\n",
        "# Predictions\n",
        "predictions = predict_sentiments(sentences)\n",
        "for text, sentiment in zip(sentences, predictions):\n",
        "    print(f\"Text: {text} => Predicted sentiment: {sentiment}\")\n"
      ]
    }
  ]
}